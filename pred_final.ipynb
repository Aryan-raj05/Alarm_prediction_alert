{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a801a4cd",
   "metadata": {},
   "source": [
    "## Load the DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d32fec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged=pd.read_csv(\"/home/sohanx1/Downloads/6th sem/SE/nokia/codes/gshare/merged_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69681a",
   "metadata": {},
   "source": [
    "## Details of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbb628",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts = merged.count()\n",
    "print(row_counts)\n",
    "merged.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb26b6",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0efec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping Alarmed Object Source System entirely as it is totally empty\n",
    "merged.drop('Alarmed Object Source System', axis=1, inplace=True)\n",
    "merged = merged.dropna(subset=['Alarm Name','Site Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7435bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Additional Text'].fillna('Unknown', inplace=True)\n",
    "merged['Is Service Affecting'].fillna(1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ac748",
   "metadata": {},
   "source": [
    "## Handling missing values for last_time_cleared columns \n",
    "\n",
    "- ### If null/empty that means it has not yet been cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b264f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['is_active'] = merged['Last Time Cleared'].isnull().astype(int)  # 1 = active, 0 = cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e1b050",
   "metadata": {},
   "source": [
    "## Removing unique rows--->columns where a records occured only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374e0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_rare_classes(merged, target_col, min_count=2):\n",
    "    \"\"\"\n",
    "    Removes rows where the target_col occurs less than min_count times.\n",
    "    Returns a cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    counts = merged[target_col].value_counts()\n",
    "    good_classes = counts[counts >= min_count].index\n",
    "    return merged[merged[target_col].isin(good_classes)].copy()\n",
    "\n",
    "for col in merged.columns:\n",
    "# Remove rare classes from Probable Cause\n",
    "    merged = remove_rare_classes(merged, target_col=col, min_count=2)\n",
    "\n",
    "# (Repeat for any other target columns if needed, e.g. Alarm Type)\n",
    "# merged = remove_rare_classes(merged, target_col='Alarm Type', min_count=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb17b2",
   "metadata": {},
   "source": [
    "## Get the Unique values from each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe59944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0: 0 unique values\n",
      "Severity: 0 unique values\n",
      "Site Name: 0 unique values\n",
      "Source System: 0 unique values\n",
      "Life Span (minutes): 0 unique values\n",
      "Alarm Name: 0 unique values\n",
      "First Time Detected: 0 unique values\n",
      "Last Time Cleared: 0 unique values\n",
      "Alarmed Object Name: 0 unique values\n",
      "Last Time Detected: 0 unique values\n",
      "Alarmed Object Type: 0 unique values\n",
      "Alarmed Object Source System: 0 unique values\n",
      "Alarm Type: 0 unique values\n",
      "Probable Cause: 0 unique values\n",
      "Specific Problem: 0 unique values\n",
      "Is Service Affecting: 0 unique values\n",
      "Alarm ID: 0 unique values\n",
      "Previous Severity: 0 unique values\n",
      "Number Of Occurrences: 0 unique values\n",
      "Additional Text: 0 unique values\n"
     ]
    }
   ],
   "source": [
    "unique_values = {col: merged[col].unique() for col in merged.columns}\n",
    "max_len = max(len(v) for v in unique_values.values())\n",
    "for k in unique_values:\n",
    "    unique_values[k] = list(unique_values[k]) + [None] * (max_len - len(unique_values[k]))\n",
    "\n",
    "unique_df = pd.DataFrame(unique_values)\n",
    "unique_df.to_csv('unique_values.csv', index=False)\n",
    "\n",
    "\n",
    "## Print No of Unique values in each columns\n",
    "for col in merged.columns:\n",
    "    print(f\"{col}: {merged[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce50f65",
   "metadata": {},
   "source": [
    "## Getting only 400k rows for training due to computing constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=pd.read_csv(\"/home/sohanx1/Downloads/6th sem/SE/nokia/codes/gshare/cleaned_merged.csv\",nrows=400000)\n",
    "dff.to_csv(\"400k_merged_cleaned_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd085c3",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d50497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import classification_report, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your cleaned dataset\n",
    "df = pd.read_csv(\"/content/400k_merged_cleaned_data.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Label Encode Categorical Features\n",
    "categorical_cols = [\n",
    "    'Severity', 'Site Name', 'Source System', 'Alarm Name',\n",
    "    'Alarmed Object Name', 'Alarmed Object Type', 'Alarm Type',\n",
    "    'Probable Cause', 'Specific Problem', 'Previous Severity'\n",
    "]\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Ensure numeric\n",
    "df['Life Span (minutes)'] = pd.to_numeric(df['Life Span (minutes)'], errors='coerce')\n",
    "df['Number Of Occurrences'] = pd.to_numeric(df['Number Of Occurrences'], errors='coerce')\n",
    "df['Is Service Affecting'] = pd.to_numeric(df['Is Service Affecting'], errors='coerce')\n",
    "df['is_active'] = pd.to_numeric(df['is_active'], errors='coerce')\n",
    "\n",
    "# Drop unneeded columns\n",
    "drop_cols = ['Unnamed: 0', 'Alarm ID', 'First Time Detected', 'Last Time Cleared', 'Last Time Detected', 'Additional Text']\n",
    "df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# --------------------------\n",
    "# 3. Step 1: Predict Next Alarm Type (Classification)\n",
    "# --------------------------\n",
    "# Inputs (exclude Alarm Type and targets to be predicted later)\n",
    "input_features_alarm_type = [\n",
    "    'Severity', 'Site Name', 'Source System', 'Alarm Name',\n",
    "    'Alarmed Object Name', 'Alarmed Object Type',\n",
    "    'Previous Severity', 'Is Service Affecting', 'Number Of Occurrences', 'is_active'\n",
    "]\n",
    "target_alarm_type = 'Alarm Type'\n",
    "\n",
    "X1 = df[input_features_alarm_type]\n",
    "y1 = df[target_alarm_type]\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42, stratify=y1)\n",
    "\n",
    "alarm_type_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "alarm_type_clf.fit(X1_train, y1_train)\n",
    "y1_pred = alarm_type_clf.predict(X1_test)\n",
    "\n",
    "print(\"\\n--- [Step 1] Alarm Type Classification Report ---\")\n",
    "print(classification_report(y1_test, y1_pred))\n",
    "\n",
    "# --------------------------\n",
    "# 4. Step 2: Predict Alarm Duration (Regression)\n",
    "# --------------------------\n",
    "# Inputs: Same as above + predicted alarm type (use true during training, predicted at test/inference)\n",
    "input_features_duration = input_features_alarm_type + ['Alarm Type']\n",
    "target_duration = 'Life Span (minutes)'\n",
    "\n",
    "X2 = df[input_features_duration]\n",
    "y2 = df[target_duration]\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "duration_reg = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "duration_reg.fit(X2_train, y2_train)\n",
    "y2_pred = duration_reg.predict(X2_test)\n",
    "\n",
    "print(\"\\n--- [Step 2] Duration Regression ---\")\n",
    "print(\"MAE:\", mean_absolute_error(y2_test, y2_pred))\n",
    "\n",
    "# --------------------------\n",
    "# 5. Step 3: Predict Probable Cause (Classification)\n",
    "# --------------------------\n",
    "# Inputs: All above + predicted duration (true during training, predicted at test/inference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Step 3: Predict Probable Cause (Classification)\n",
    "# Inputs: All above + predicted duration (true during training, predicted at test/inference)\n",
    "\n",
    "\n",
    "\n",
    "input_features_cause = input_features_alarm_type + ['Alarm Type', 'Life Span (minutes)']\n",
    "target_cause = 'Probable Cause'\n",
    "# ----Removing rare class-------\n",
    "value_counts = df[target_cause].value_counts()\n",
    "df_filtered = df[df[target_cause].isin(value_counts[value_counts > 1].index)]\n",
    "\n",
    "\n",
    "X3 = df_filtered[input_features_cause]\n",
    "y3 = df_filtered[target_cause]\n",
    "le_cause = LabelEncoder()\n",
    "y3 = le_cause.fit_transform(y3)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(\n",
    "    X3, y3, test_size=0.4, random_state=42, stratify=y3\n",
    ")\n",
    "\n",
    "\n",
    "cause_clf = XGBClassifier(tree_method=\"hist\", use_label_encoder=False, eval_metric='mlogloss')\n",
    "cause_clf.fit(X3_train, y3_train)\n",
    "y3_pred = cause_clf.predict(X3_test)\n",
    "\n",
    "print(\"\\n--- [Step 3] Probable Cause Classification Report ---\")\n",
    "print(classification_report(y3_test, y3_pred))\n",
    "\n",
    "# 6. How to Chain at Inference (on new sample)\n",
    "# Let's say you have a new incoming alarm row: new_alarm_row (pd.DataFrame with same columns)\n",
    "# You would do:\n",
    "#   1. Predict Alarm Type with alarm_type_clf\n",
    "#   2. Add that to features, predict Duration with duration_reg\n",
    "#   3. Add both, predict Probable Cause with cause_clf\n",
    "\n",
    "\n",
    "# Decode predictions (optional, for interpretability)\n",
    "decoded_alarm_type = encoders['Alarm Type'].inverse_transform([pred_alarm_type])[0]\n",
    "decoded_cause = encoders['Probable Cause'].inverse_transform([pred_cause])[0]\n",
    "\n",
    "print(\"\\n--- Chained Inference Example ---\")\n",
    "print(f\"Predicted Next Alarm Type: {decoded_alarm_type}\")\n",
    "print(f\"Predicted Duration: {pred_duration:.2f} minutes\")\n",
    "print(f\"Predicted Probable Cause: {decoded_cause}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sohanpython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
